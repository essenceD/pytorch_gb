{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dcc59a9-8b09-49cb-83f5-5af924132759",
   "metadata": {},
   "source": [
    "1. Попробуйте обучить нейронную сеть GRU/LSTM для предсказания сентимента сообщений с твитера на примере https://www.kaggle.com/datasets/arkhoshghalb/twitter-sentiment-analysis-hatred-speech\n",
    "\n",
    "2. Опишите, какой результат вы получили? Что помогло вам улучшить ее точность?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f59e75d-a731-4d7c-8a7b-9a771886eb47",
   "metadata": {},
   "source": [
    "# Контекст\n",
    "Цель этой задачи — обнаружить язык ненависти в твитах. Для простоты мы говорим, что твит содержит разжигание ненависти, если с ним связаны расистские или сексистские настроения. Итак, задача состоит в том, чтобы отделить расистские или сексистские твиты от других твитов.\n",
    "\n",
    "Формально, учитывая обучающую выборку твитов и ярлыков, где метка «1» означает, что твит является расистским/сексистским, а метка «0» означает, что твит не является расистским/сексистским, ваша цель состоит в том, чтобы предсказать метки в тестовом наборе данных.\n",
    "\n",
    "# Содержание\n",
    "Полные тексты твитов снабжены метками для обучающих данных.\n",
    "Имя пользователя упомянутых пользователей заменяется на @user ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "949df132-31b8-4268-9879-1da2e623ac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchmetrics\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aac5544-d2ce-4938-aedd-3bffef191dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24c25083-fb70-4f33-9f10-ccefb9bb13d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_valid = train_test_split(train_df, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31c9b284-4b22-46d8-a25c-de7ce706849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(stopwords.words('english'))\n",
    "exclude = set(punctuation)\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "698de795-6d36-4437-9319-8bbc0c2c4a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(txt):\n",
    "    #преобразуем входные данные в строку\n",
    "    txt = str(txt)\n",
    "    #удяляем знаки пунктуации\n",
    "    txt = \"\".join(c for c in txt if c not in exclude)\n",
    "    #приводим все к нижнему регистру\n",
    "    txt = txt.lower()\n",
    "    #лематизация и удаление стоп-слов\n",
    "    txt = [lemmatizer.lemmatize(w) for w in txt.split() if w not in sw]\n",
    "    return \" \".join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "680dae59-db11-4d0e-b174-f1292458d28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 23971/23971 [00:02<00:00, 11143.43it/s]\n",
      "100%|██████████████████████████████████████████████████| 7991/7991 [00:00<00:00, 25941.48it/s]\n",
      "100%|████████████████████████████████████████████████| 17197/17197 [00:00<00:00, 24529.49it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "df_train['tweet'] = df_train['tweet'].progress_apply(preprocess_text)\n",
    "df_valid['tweet'] = df_valid['tweet'].progress_apply(preprocess_text)\n",
    "test_df['tweet'] = test_df['tweet'].progress_apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84f63c1c-0576-4217-8f6f-4895afb330be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weekend',\n",
       " 'world',\n",
       " 'really',\n",
       " 'going',\n",
       " 'bonkers',\n",
       " 'really',\n",
       " 'bad',\n",
       " 'atm',\n",
       " 'shooting',\n",
       " 'deathstroke']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus = \" \".join(df_train[\"tweet\"])\n",
    "train_corpus = train_corpus.lower()\n",
    "tokens = word_tokenize(train_corpus)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6625261e-5a3d-4963-a065-c0e7f2c5aa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_filtered = [word for word in tokens if word.isalnum()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65d8229d-d7e6-464f-8662-879e24d7e02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = FreqDist(tokens_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "910020b2-b4bd-4cf2-89c5-0449f4ddc0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "max_len = 10\n",
    "num_classes = 1\n",
    "\n",
    "# обучение\n",
    "epochs = 10\n",
    "batch_size = 512\n",
    "print_batch_n = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74b8e5cc-e97e-4ced-9b06-884aed8655b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19999,\n",
       " ['user', 'love', 'day', 'u', 'happy', 'amp', 'time', 'life', 'im', 'today'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_filtered_top = [pair[0] for pair in dist.most_common(max_words-1)]  # -1 - padding\n",
    "len(tokens_filtered_top), tokens_filtered_top[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "257593e9-0521-4885-acc5-e6ed0732857b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {v: k for k, v in dict(enumerate(tokens_filtered_top,1)).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb00f968-971a-4a27-972e-e4c37ac1fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(text, maxlen):\n",
    "    result = []\n",
    "    tokens = word_tokenize(text.lower())  #токенизация\n",
    "    tokens_filtered = [word for word in tokens if word.isalnum()] #фильтруем (только буквы и цифры)\n",
    "    for word in tokens_filtered:\n",
    "        if word in vocabulary:\n",
    "            result.append(vocabulary[word]) #если слово в топе токенов, то добавляем его индекс в результат\n",
    "\n",
    "    padding = [0] * (maxlen-len(result)) #нули дополняющие до maxlen\n",
    "    return result[-maxlen:] + padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6007b37-bf5e-4c13-81b9-6f551ae96197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23971, 10), (7991, 10), (17197, 10))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.asarray([text_to_sequence(text, max_len) for text in df_train[\"tweet\"]], dtype=np.int32)\n",
    "x_valid = np.asarray([text_to_sequence(text, max_len) for text in df_valid[\"tweet\"]], dtype=np.int32)\n",
    "x_test = np.asarray([text_to_sequence(text, max_len) for text in test_df[\"tweet\"]], dtype=np.int32)\n",
    "\n",
    "x_train.shape, x_valid.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c40e314-d611-40c9-8f60-68359c4c1f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataWrapper(Dataset):\n",
    "    def __init__(self, data, target, transform=None):\n",
    "        self.data = torch.from_numpy(data).long() #преобразуем в целочисленный тензор\n",
    "        self.target = torch.from_numpy(target).long() #преобразуем в целочисленный тензор\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]  #индексация данных\n",
    "        y = self.target[index]  #индексация целевой переменной\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "            \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "471041e7-6700-4b0d-a142-102e8652c64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataWrapper(x_train, df_train['label'].values)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_dataset = DataWrapper(x_valid, df_valid['label'].values)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c2ea089-173e-485e-835a-2d7de6af9636",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUFixedLen(nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=128, use_last=True):\n",
    "        super().__init__()\n",
    "        self.use_last = use_last\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        gru_out, ht = self.gru(x)\n",
    "       \n",
    "        if self.use_last:\n",
    "            last_tensor = gru_out[:,-1,:]\n",
    "        else:\n",
    "            # use mean\n",
    "            last_tensor = torch.mean(gru_out[:,:], dim=1)\n",
    "    \n",
    "        out = self.linear(last_tensor)\n",
    "        return torch.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9b7d310-63f6-4af0-b4ef-b9431ecaf792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f8b9885-1cde-4c45-91ce-498978bea5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRUFixedLen(max_words).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c979f32f-75e7-4e26-b6d2-4a51ac22c3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]. Step [47/47]. Loss: 0.192. Acc: 0.940. Test loss: 0.085. Test acc: 0.945\n",
      "Epoch [2/10]. Step [47/47]. Loss: 0.117. Acc: 0.951. Test loss: 0.235. Test acc: 0.949\n",
      "Epoch [3/10]. Step [47/47]. Loss: 0.105. Acc: 0.960. Test loss: 0.006. Test acc: 0.954\n",
      "Epoch [4/10]. Step [47/47]. Loss: 0.113. Acc: 0.966. Test loss: 0.499. Test acc: 0.952\n",
      "Epoch [5/10]. Step [47/47]. Loss: 0.091. Acc: 0.972. Test loss: 0.005. Test acc: 0.953\n",
      "Epoch [6/10]. Step [47/47]. Loss: 0.058. Acc: 0.977. Test loss: 0.424. Test acc: 0.957\n",
      "Epoch [7/10]. Step [47/47]. Loss: 0.082. Acc: 0.981. Test loss: 0.005. Test acc: 0.955\n",
      "Epoch [8/10]. Step [47/47]. Loss: 0.043. Acc: 0.984. Test loss: 0.003. Test acc: 0.956\n",
      "Epoch [9/10]. Step [47/47]. Loss: 0.029. Acc: 0.987. Test loss: 0.809. Test acc: 0.956\n",
      "Epoch [10/10]. Step [47/47]. Loss: 0.037. Acc: 0.988. Test loss: 0.003. Test acc: 0.952\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "th = 0.5\n",
    "\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "\n",
    "for epoch in range(epochs): \n",
    "    model.train() \n",
    "    running_items, running_right = 0.0, 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # обнуляем градиент\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels.float().view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # подсчет ошибки на обучении\n",
    "        loss = loss.item()\n",
    "        running_items += len(labels)\n",
    "        # подсчет метрики на обучении\n",
    "        pred_labels = torch.squeeze((outputs > th).int())\n",
    "        running_right += (labels == pred_labels).sum()\n",
    "        \n",
    "    # выводим статистику о процессе обучения\n",
    "    model.eval()\n",
    "    \n",
    "    print(f'Epoch [{epoch + 1}/{epochs}]. ' \\\n",
    "          f'Step [{i + 1}/{len(train_loader)}]. ' \\\n",
    "          f'Loss: {loss:.3f}. ' \\\n",
    "          f'Acc: {running_right / running_items:.3f}', end='. ')\n",
    "    running_loss, running_items, running_right = 0.0, 0.0, 0.0\n",
    "    train_loss_history.append(loss)\n",
    "\n",
    "    # выводим статистику на тестовых данных\n",
    "    test_running_right, test_running_total, test_loss = 0.0, 0.0, 0.0\n",
    "    for j, data in enumerate(valid_loader):\n",
    "        test_labels = data[1].to(device)\n",
    "        test_outputs = model(data[0].to(device))\n",
    "        \n",
    "        # подсчет ошибки на тесте\n",
    "        test_loss = criterion(test_outputs, test_labels.float().view(-1, 1))\n",
    "        # подсчет метрики на тесте\n",
    "        test_running_total += len(data[1])\n",
    "        pred_test_labels = torch.squeeze((test_outputs > th).int())\n",
    "        test_running_right += (test_labels == pred_test_labels).sum()\n",
    "    \n",
    "    test_loss_history.append(test_loss.item())\n",
    "    print(f'Test loss: {test_loss:.3f}. Test acc: {test_running_right / test_running_total:.3f}')\n",
    "            \n",
    "print('Training is finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf22b222-8cb6-4092-bdc5-57fb185a92b4",
   "metadata": {},
   "source": [
    "Точность с первого раза получилась довольно хорошей, видно, что переобучение отсутствует. Выбрал ГРУ потому, что хотел проверить как модель работает в балансном режиме. ЛСТМ будет не хуже, думаю."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bdc980-ec92-49dc-a52a-3994c8c40025",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
